# Azure AI Evaluation Tryout

A sample project demonstrating the usage of Azure AI Evaluation SDK to evaluate AI model responses.

## To do

- [ ] We need to determine how we represent the data we provide as context for the LLM; json? csv? a formatted string? I think a fixed formatted string might be best
- [ ] Local app evaluation | [link](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk)
- [ ] Custom app evaluation | [link](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk#custom-evaluators)
- [ ] Look into evaluating agents | [link](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)
- [ ] Look into cloud evaluations through code | [link](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation)
- [ ] Try out whether Simulator through code can generate solid input data | [link](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/simulator-interaction-data)

## Overview

This project provides example code for evaluating AI-generated text and responses using Azure's AI evaluation tools. It demonstrates various evaluation techniques including:

- NLP metrics (BLEU, F1, GLEU, METEOR, ROUGE)
- AI-assisted quality evaluation (groundedness, relevance, coherence, fluency, similarity)
- Risk and safety evaluation (content safety, self-harm, hate/unfairness, indirect attack, violence, sexual content, code vulnerability)
- Composite evaluators (QA, content safety)

## Requirements

- Python 3.12+
- Azure AI Foundry project (<https://ai.azure.com>)

## Installation

1. Install uv

   ```bash
   # On macOS/Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # On Windows (PowerShell)
   powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
   ```

   See the [uv installation documentation](https://docs.astral.sh/uv/getting-started/installation/) for more options.

2. Sync dependencies (this automatically creates a virtual environment)

   ```bash
   uv sync
   ```

## Configuration

Create a `.env` file in the root directory with the following variables (these can all be found in the Azure AI Foundry project you should create here: <https://ai.azure.com>):

```bash
AZURE_OPENAI_DEPLOYMENT=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_VERSION=

AZURE_FOUNDRY_SUBSCRIPTION_ID=
AZURE_FOUNDRY_RESOURCE_GROUP_NAME=
AZURE_FOUNDRY_PROJECT_NAME=
```

## Usage

Run the sample evaluator script:

```bash
uv run evaluator-samples.py
```

## Azure AI Foundry Evaluations (UI)

- There are three types of evaluators. The AI-assisted quality metrics seem most interesting for this project.
    1. AI-assisted quality metrics (groundedness, relevance, coherence, fluence, and similarity)
    2. NLP metrics (F1, BLEU, GLEU, METEOR, ROUGE)
    3. AI-assisted risk and safety metrics (self-harm, hateful, violent, sexual, protected material, indirect attack)
- For evaluation datasets (must be `.jsonl` or `.csv`) there are four columns:
  - **`query`**

    The user’s input prompt or the scenario that the model was asked to respond to.

  - **`ground_truth`**

    The ideal or reference response. What a correct, policy-compliant answer should look like for that scenario.

  - **`response`**

    The actual snippet generated by the model under test. This is what you evaluate against the ground truth (and against the various safety metrics).

  - **`context`**

    A label that highlights the content theme or category being tested.

- **Automatic evaluations**
  - “Model and prompt”: allows you to define a system prompt and then provide an evaluation dataset. This is very suitable for testing individual agents.
    - Contains a feature that allows you to use a GPT model to generate sample question/answer pairs, but it is limited to simple one-sentence questions, not large data inputs; so it is best to generate that ourselves.
  - “Dataset”: allows you to evaluate an existing output dataset. This can be very useful for analysing the response to a full loop through our system, where the “query” would be all of the data the decision is based on. The following image was generated with the dataset feature on the `/datasets/example.csv` dataset.
    - Screenshot: ![image.png](images/ai-foundry-evaluation.png)

  - “Prompt flow”: seems like an evaluator for low-code AI flows.
- **Manual evaluations** allow you to give thumbs up or down manually for your query/response dataset, but are not very useful beyond that. Maybe useful as input for finetuning?
- The evaluation SDK is still quite new, and I got [this error](https://github.com/Azure/azure-sdk-for-python/issues/40992) when doing an install of azure-ai-evaluations

## Sample Datasets

The `datasets` directory contains example data for testing the evaluators in the Azure AI Foundry:

- `scenarios.jsonl`: Sample scenarios for evaluation
- `example.csv`: Example dataset in CSV format

## Documentation

For more information about Azure AI Evaluation, refer to:

- [Azure AI Evaluation SDK Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk)
- [Azure AI Evaluation API Reference](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python-preview)
