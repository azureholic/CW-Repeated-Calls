# Azure AI Evaluation Tryout

A sample project demonstrating the usage of Azure AI Evaluation SDK to evaluate AI model responses.

## Requirements

- Python 3.12+
- Azure AI Foundry project (<https://ai.azure.com>)

## Installation

1. Install uv

   ```bash
   # On macOS/Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # On Windows (PowerShell)
   powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
   ```

   See the [uv installation documentation](https://docs.astral.sh/uv/getting-started/installation/) for more options.

2. Install dependencies

   ```bash
   # Create a virtual environment
   uv venv

   # Activate the virtual environment
   source .venv/bin/activate

   # On Windows (PowerShell)
   .venv\Scripts\Activate.ps1

   # Install dependencies
   uv sync
   ```

## Configuration

Create a `.env` file in the root directory with the following variables (these can all be found in the Azure AI Foundry project you should create here: <https://ai.azure.com>):

```bash
AZURE_OPENAI_DEPLOYMENT=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_VERSION=

AZURE_FOUNDRY_SUBSCRIPTION_ID=
AZURE_FOUNDRY_RESOURCE_GROUP_NAME=
AZURE_FOUNDRY_PROJECT_NAME=
```

## Usage

Run the sample evaluator script (demonstrates various evaluators with simple examples):

```bash
uv run scripts/0_evaluator-samples.py
```

Run the local evaluation script (evaluates dataset responses using different models and makes the results available in the Foundry UI):

```bash
uv run scripts/1_local_evaluation.py
```

## Findings

### Local evaluations

- CSV and JSONL datasets are supported.
- Reasoning models do not seem to be supported; I received the following error trying to use DeepSeek-R1 and o4-mini:

```bash
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
```

### Azure AI Foundry Evaluations (UI)

- There are three types of evaluators. The AI-assisted quality metrics seem most interesting for this project.
  1. AI-assisted quality metrics (groundedness, relevance, coherence, fluence, and similarity)
  2. NLP metrics (F1, BLEU, GLEU, METEOR, ROUGE)
  3. AI-assisted risk and safety metrics (self-harm, hateful, violent, sexual, protected material, indirect attack)
- For evaluation datasets (must be `.jsonl` or `.csv`) there are four columns:

  - **`query`**

    The user's input prompt or the scenario that the model was asked to respond to.

  - **`ground_truth`**

    The ideal or reference response. What a correct, policy-compliant answer should look like for that scenario.

  - **`response`**

    The actual snippet generated by the model under test. This is what you evaluate against the ground truth (and against the various safety metrics).

  - **`context`**

    A label that highlights the content theme or category being tested.

- **Automatic evaluations**

  - "Model and prompt": allows you to define a system prompt and then provide an evaluation dataset. This is very suitable for testing individual agents.
    - Contains a feature that allows you to use a GPT model to generate sample question/answer pairs, but it is limited to simple one-sentence questions, not large data inputs; so it is best to generate that ourselves.
  - "Dataset": allows you to evaluate an existing output dataset. This can be very useful for analysing the response to a full loop through our system, where the "query" would be all of the data the decision is based on. The following image was generated with the dataset feature on the `/datasets/example.csv` dataset.

    - Screenshot: ![image.png](images/ai-foundry-evaluation.png)

  - "Prompt flow": seems like an evaluator for low-code AI flows.
  - For some reason, the Phi-4 model is not available in the UI.

- **Manual evaluations** allow you to give thumbs up or down manually for your query/response dataset, but are not very useful beyond that. Maybe useful as input for finetuning?
- The evaluation SDK is still quite new, and I got [this error](https://github.com/Azure/azure-sdk-for-python/issues/40992) when doing an install of azure-ai-evaluations

## Sample Datasets

The `datasets` directory contains example data for testing the evaluators in the Azure AI Foundry:

- `scenarios.jsonl`: Sample scenarios for evaluation
- `example.csv`: Example dataset in CSV format

## Documentation

For more information about Azure AI Evaluation, refer to:

- [Azure AI Evaluation SDK Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk)
- [Azure AI Evaluation API Reference](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python-preview)
