# Azure AI Evaluation Tryout

A sample project demonstrating the usage of Azure AI Evaluation SDK to evaluate AI model responses.

## Requirements

- Python 3.12+
- Azure AI Foundry project (<https://ai.azure.com>)

## Installation

1. Install uv

   ```bash
   # On macOS/Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # On Windows (PowerShell)
   powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
   ```

   See the [uv installation documentation](https://docs.astral.sh/uv/getting-started/installation/) for more options.

2. Install dependencies

   ```bash
   # Create a virtual environment
   uv venv

   # Activate the virtual environment
   source .venv/bin/activate

   # On Windows (PowerShell)
   .venv\Scripts\Activate.ps1

   # Install dependencies
   uv sync
   ```

## Configuration

Create a `.env` file in the root directory with the following variables (these can all be found in the Azure AI Foundry project you should create here: <https://ai.azure.com>):

```bash
AZURE_OPENAI_DEPLOYMENT=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_VERSION=

AZURE_FOUNDRY_SUBSCRIPTION_ID=
AZURE_FOUNDRY_RESOURCE_GROUP_NAME=
AZURE_FOUNDRY_PROJECT_NAME=
```

## Usage

Run the sample evaluator script (demonstrates various evaluators with simple examples):

```bash
uv run scripts/0_evaluator-samples.py
```

Run the local evaluation script (evaluates dataset responses using different models and makes the results available in the Foundry UI):

```bash
uv run scripts/1_local_evaluation.py
```

## Findings

### General information: evaluator types

There multiple categories of ready-made evaluators. The AI-assisted quality metrics appear most promising for the repeated calls project. See the [API reference](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python) for a detailed description for each evaluator.

1. **AI-Assisted Quality Metrics:**
   - Groundedness
   - Relevance
   - Coherence
   - Fluency
   - Similarity
   - Retrieval (suitable for RAG evaluations)
2. **NLP Metrics:**
   - F1
   - BLEU
   - GLEU
   - METEOR
   - ROUGE
3. **AI-Assisted Risk and Safety Metrics:**
   - Self-harm
   - Hateful
   - Violent
   - Sexual
   - Protected material
   - Indirect attack
4. **Agent Evaluators:**
   - Intent Resolution
   - Tool Call Accuracy
   - Task Adherence
5. **Azure OpenAI Evaluators:**
   - Label grader
   - String checker
   - Text similarity
6. You also have the option to create [custom evaluators](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/custom-evaluators).

### Dataset requirements

Evaluation datasets can be provided in either `.jsonl` or `.csv` format. Two primary structures are supported:

#### A. Single-Turn Chats

This format is suitable for evaluating individual query-response interactions.

- **Columns:** (each evaluator requires different columns)

  - `query`: The user's input prompt or the scenario the model responded to.
  - `ground_truth`: The ideal or reference response, typically human-generated.
  - `response`: The response generated by the AI model to the query.
  - `context`: The source information on which the generated response is based (grounding information).

- **Example** (single-turn chat `.jsonl` format, one JSON object per line):

  ```json
  {"query":"Customer reports that her RoboTrim X keeps stalling mid-session.","ground_truth":"Yes, this is a repeated call.","response":"The customer has previously reported that the RoboTrim X stalls during operation. The current call is about the same malfunction, with the product referenced by name and identical symptoms. This matches the prior issue and falls within a few days of the last call.","context":"Previous call (3 days ago): 'RoboTrim X keeps pausing randomly.' Summary: Diagnosed intermittent motor issue, pending technician visit."}
  {"query":"Caller says her garden mower wonâ€™t start this morning.","ground_truth":"No, this is not a repeated call.","response":"Although this call is about a garden mower, the last call was regarding a billing dispute for a fertilizer subscription. There is no overlap in the product or issue being discussed. Therefore, this is not a repeated call.","context":"Previous call (5 days ago): 'I was charged twice for the premium fertilizer plan.' Summary: Resolved overbilling complaint."}
  ```

#### B. Conversations

This format is designed for evaluating multi-turn dialogues.

- **Structure:** each record in a `.jsonl` file is a JSON object containing a `conversation` key. The value is an object with a `messages` array. Each message object in the array includes:

  - `role`: indicates the speaker (e.g., `system`, `user`, `assistant`).
  - `content`: the text of the message.
  - `context` (optional, typically for `assistant` messages): provides grounding information for the assistant's response.

- **Example:** (here in `json` format for readability)

  ```json
  {
    "conversation": {
      "messages": [
        {
          "role": "system",
          "content": "You are a customer service expert system. Determine if an incoming call is a repeat call. A repeat call concerns the same issue with the same product within the past month. Use call reasons and summaries to compare current and previous calls."
        },
        {
          "role": "user",
          "content": "## Customer Info\nID: 12\nName: Jamie Wu\n\n## Current Call\nCall Description: My AutoMow 3000 still shuts off mid-mow\nDate: 2025-05-10\n\n## Previous Calls\n- 2025-05-07: AutoMow 3000 stops halfway through mowing. Summary: Issue confirmed, troubleshooting started.\n- 2025-05-03: AutoMow 3000 keeps turning off. Summary: Customer reported repeated shutdowns. Asked to monitor behavior.\n\nIs this a repeat call?"
        },
        {
          "role": "assistant",
          "content": "{\"customer_id\":12,\"is_repeated_call\":true,\"conclusion\":\"Yes, this is a repeat call.\",\"analysis\":\"All calls are about the AutoMow 3000 shutting down mid-operation. The current call continues the same unresolved issue reported in earlier calls.\"}",
          "context": "## Customer Info\nID: 12\nName: Jamie Wu\n\n## Current Call\nCall Description: My AutoMow 3000 still shuts off mid-mow\nDate: 2025-05-10\n\n## Previous Calls\n- 2025-05-07: AutoMow 3000 stops halfway through mowing. Summary: Issue confirmed, troubleshooting started.\n- 2025-05-03: AutoMow 3000 keeps turning off. Summary: Customer reported repeated shutdowns. Asked to monitor behavior.\n\nIs this a repeat call?"
        }
      ]
    }
  }
  ```

### Evaluation approaches and considerations

#### A. Evaluations through code

Assuming the goal is to compare the quality of responses between different models:

- **Evaluation strategies:**

  - **Continuously evaluate production:** involves tracing, automatic evaluations, and A/B testing. This approach can be expensive (although sampling can be used to reduce costs).
    - A guide on how to set this up is provided [here](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/continuous-evaluation-agents). I made an honest attempt at following the guide to make this work but I was not successful due to the quality of the documentation (or I missed something).
  - **Periodically evaluate production:** save queries and responses/conversations to a database, then perform data preparation and feed them to an evaluation script. This adds storage overhead.
    - I think this is the most practical approach for the repeated calls project.
  - **Evaluate test scenarios:** run predefined test scenarios with different models. The risk is that these scenarios might not accurately represent production conversations.

- **Evaluation granularity (individual agent responses vs. whole loop output):**

  - **Individual agent responses:**
    - Offers more granular insights.
    - Can be done using `conversation` objects (as shown above).
    - Comparing models at the agent level is useful, as different agents might have different model requirements.
    - The `single-turn chat` format can also be adapted for individual agent response evaluation.
  - **Whole loop output:**
    - Simpler to implement but may be less useful for pinpointing specific areas for improvement.
    - Can be done using `single-turn chat` objects (e.g., `query` encapsulates all input, `response` is the final output, `context` includes intermediate steps/prompts).
    - **Challenge:** Identifying where to make improvements if the full loop response is insufficient.
    - **Note on groundedness:** The groundedness evaluator only requires `context` and `response`; so `ground_truth` is not always a necessity for evaluations.

- **Most interesting evaluators (for code-based evaluation):**

  - Relevance and Groundedness
    - For these, I recommend generating conversational datasets since these evaluators don't require a ground truth.
  - Similarity
    - For this, I recommend generating single-turn chat datasets since this evaluator requires a ground truth.
  - Perhaps custom evaluators would be useful for targeting more specifically whether a wrong or right answer is given for any given step (e.g., CorrectnessEvaluator).

- **Data preparation:**

  - A data preparation script will be needed to convert stored chat conversations into the expected `.jsonl` format (either conversations or single-turn chats).
  - For single-turn evaluations, careful consideration is needed for structuring the `context` (e.g., where/if to include system prompts) and defining the `query`.

- **Limitations encountered:**

  - **Reasoning models (e.g., DeepSeek-R1, o4-mini) seem unsupported** for evaluation via the tested script, yielding an error:

    ```bash
    openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
    ```

#### B. Azure AI Foundry Evaluations (UI)

The Azure AI Foundry provides a user interface for conducting evaluations.

- **Automatic Evaluations:**

  - **"Model and prompt":**
    - Well-suited for testing individual agents. Allows defining a system prompt and then providing an evaluation dataset.
    - Contains a feature to generate sample question/answer pairs using a GPT model, but this is limited to simple one-sentence questions and not ideal for large data inputs. I recommended to generate such data independently.
  - **"Dataset":**
    - Highly useful for analyzing a response dataset. The "query" can represent all data the decision is based on.
    - Image: ![ai-foundry-evaluation](images/ai-foundry-evaluation.png)
    - It is also possible to compare evaluation results between different models. This can be used to compare different models as judges for the same dataset or to compare the response quality of different models.
    - Image: ![ai-foundry-evaluation-comparison](images/ai-foundry-evaluation-comparison.png)
  - **"Prompt flow":** Appears to be an evaluator designed for low-code AI flows.
  - **Note:** The Phi-4 model was not available in the Foundry UI at the time of testing, though it was accessible when running evaluation scripts locally.

- **Manual Evaluations:**

  - Allows you to manually give thumbs up or down for query/response datasets.
  - Not very useful beyond basic feedback collection. Could potentially serve as input for fine-tuning models.

- **SDK Issue:**
  - The evaluation SDK is relatively new. I encountered an installation error: [GitHub Issue #40992](https://github.com/Azure/azure-sdk-for-python/issues/40992) (`azure-ai-evaluations` installation).

### Key takeaways for the repeated calls project

- AI-assisted quality metrics like groundedness, relevance, and similarity are highly useful for evaluating the quality of responses.
- Dataset formats for single-turn chats and conversations are both applicable, depending on whether individual agent turns or the whole loop is to be evaluated.
- Programmatic evaluation (via code/SDK) offers flexibility for integration but requires setting up data preparation pipelines.
- The Azure AI Foundry UI provides a user-friendly alternative, especially the "Dataset" feature for both single-turn and conversation evaluations and the "Model and prompt" feature for agent-specific tests.
- Be aware of potential model compatibility issues (e.g., with reasoning models in certain evaluation setups) and the maturity of the SDK, which is still evolving.

## Additional opportunities to explore

Here are some promising areas that could enhance the evaluation strategy but were not yet explored:

- **Simulator for generating datasets**
  The simulator available via the Azure AI Evaluation SDK appears to have more potential than the simplistic one in the Foundry UI. It could help in creating realistic, large-scale datasets for training and evaluation.
  ðŸ‘‰ [Simulator documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/simulator-interaction-data)

- **CI/CD evaluations**
  Integrating evaluation workflows directly into development pipelines could enable continuous quality control. These tools allow running evaluations automatically as part of GitHub Actions or Azure DevOps pipelines.

  - [GitHub Actions integration](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluation-github-action)
  - [Azure DevOps integration](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluation-azure-devops)

- **Red teaming for risk and safety**
  While this is outside quality evaluation, red teaming is useful for testing models against harmful, unsafe, or policy-violating content. Particularly useful for models in sensitive domains.
  ðŸ‘‰ [Red teaming concepts](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/ai-red-teaming-agent)

## Documentation

For more information about Azure AI Evaluation, refer to:

- [Azure AI Evaluation SDK Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk)
- [Azure AI Evaluation API Reference](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python-preview)
